{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part  4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import tqdm\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "colors = [\n",
    "    (255, 0, 0),\n",
    "    (0, 255, 0),\n",
    "    (0, 0, 255),\n",
    "    (255, 255, 0),\n",
    "    (255, 0, 255),\n",
    "    (0, 255, 255),\n",
    "    (255, 192, 203),\n",
    "    (0, 125, 125),\n",
    "    (255, 125, 125),\n",
    "    (125, 255, 255),\n",
    "    (255, 255, 125),\n",
    "    (125, 255, 125),\n",
    "    (255, 125, 255),\n",
    "    (125, 125, 125),\n",
    "    (125, 125, 0),\n",
    "    (0, 125, 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 \n",
    "\n",
    "(3) In the given sequence (00-29) pick 15 cells. These cells are traced cells over the\n",
    "sequence of the 30 images. Indicate your choices with a label/number in the initial\n",
    "image. These labels are the result from a segmentation operation that successfully\n",
    "find all the relevant, i.e. 15, cells in the image(s). Develop, apply, explain and motivate\n",
    "your segmentation procedure.\n",
    "\n",
    "The segmentation procedure segments all seperate cells in the image. The tracking algorithm (4.1.3) is responsible for finding the 15 relevant cells from the segmentation results. The segmentation procedure itself is quite simple, the images are normalized using normalization from cv2:\n",
    "\n",
    "cv2.normalize(image, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)  \n",
    "\n",
    "After that, opening is applied to remove the smallest shapes that are too small to be seperate cells:\n",
    "\n",
    "cv2.morphologyEx(image, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_RECT, (9,9)))\n",
    "\n",
    "A structuring element of (9x9) was selected after carefull testing. This removed all the smallest shapes without corrupting the cells. After the opening, the image was adaptively thresholded using Otsu thresholding: \n",
    "\n",
    "cv2.threshold(image, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "\n",
    "This obtained a binary image, which was then used for finding the separate contours:\n",
    "\n",
    "cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "These contours are the segmented cells. The following code in this notebook will display the relevant visualizations for this procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all 30 images from the controll experiment\n",
    "images_ctrl = [np.mean(plt.imread(f\"images/MTLn3-Ctrl00{'0' + str(i) if i < 10 else i}.tif\"), -1).astype(np.uint16) for i in range(30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_eight(images, start=0):\n",
    "    # Create a figure and axes for the subplots\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "\n",
    "    # Iterate over the image paths and corresponding axes\n",
    "    for i, (image, ax) in enumerate(zip(images, axes.flat)):\n",
    "        # Load and display the image on the current axis\n",
    "        ax.imshow(image, cmap=\"gray\")\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'Image {start+i+1}')\n",
    "\n",
    "    # Adjust the spacing between subplots\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Show the figure\n",
    "    plt.show()\n",
    "\n",
    "plt.title(\"First image - no processing\")    \n",
    "plt.imshow(images_ctrl[0], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_segmentation(original, contours, number=1, white=False, show=True):\n",
    "        color_image = np.stack([original.copy()]*3, axis=-1).astype(np.uint16)\n",
    "        original_image = copy.deepcopy(original)\n",
    "\n",
    "        for i, (contour, color) in enumerate(zip(contours, colors)):\n",
    "            plot_color = (255, 255, 255) if white else color\n",
    "            cv2.drawContours(color_image, [contour], -1, color=plot_color, thickness=cv2.FILLED)\n",
    "            cx, cy = min(contour, key=lambda l: l[0][0])[0][0], min(contour, key=lambda l: l[0][1])[0][1]\n",
    "\n",
    "            cv2.putText(color_image, text= str(i+1), org=(cx,cy), fontFace= cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(0,0,0), \n",
    "                        thickness=2, lineType=cv2.LINE_AA)\n",
    "            cv2.putText(original_image, text= str(i+1), org=(cx,cy), fontFace= cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(255, 255, 255), \n",
    "                        thickness=2, lineType=cv2.LINE_AA)\n",
    "        \n",
    "        if show:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "            axes[0].imshow(color_image)\n",
    "            axes[1].imshow(original_image, cmap=\"gray\")\n",
    "            axes[0].set_title(f\"Selected cells in segmented image {number}\")\n",
    "            axes[1].set_title(f\"Selected cells in original image {number}\")\n",
    "            plt.show()\n",
    "        \n",
    "        return color_image, original_image\n",
    "    \n",
    "def plot_all_segmentation_white(original, contours):\n",
    "        original_image = copy.deepcopy(original)\n",
    "\n",
    "        for contour in contours:\n",
    "            cv2.drawContours(original_image, [contour], -1, color=(255, 255, 255), thickness=cv2.FILLED)\n",
    "        \n",
    "        plt.imshow(original_image)\n",
    "        plt.title(\"Segmentation result on first image\")\n",
    "        plt.show()\n",
    "\n",
    "def segment_cells(image, plot=False):\n",
    "    \"\"\"\n",
    "    Segment different cells in image. Note that many pipelines were tried, and just normalizing and thresholding worked best\n",
    "    \"\"\"\n",
    "    original = image.copy()\n",
    "    # Normalize\n",
    "    image = cv2.normalize(image, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)    \n",
    "        \n",
    "    # Open the image to remove small shapes\n",
    "    image = cv2.morphologyEx(image, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_RECT, (9,9)))\n",
    "    \n",
    "    # Threshold the image\n",
    "    _, image = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "\n",
    "    # Convert to unsigned 8-bit integers\n",
    "    image = image.astype(np.uint8)\n",
    "\n",
    "    # Find the contours\n",
    "    contours, _ = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Find the largest contours\n",
    "    largest = sorted(contours, key=cv2.contourArea, reverse=True)[:15]\n",
    "    \n",
    "    # Plot the segmented and labeled images\n",
    "    if plot:\n",
    "        plot_segmentation(original, largest)\n",
    "\n",
    "    return contours\n",
    "\n",
    "plot_all_segmentation_white(images_ctrl[0], segment_cells(images_ctrl[0], plot=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_cells(images_ctrl[0], plot=True)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2\n",
    "Five labeled cells, with numbers (2, 7, 9, 11, 15) are manually tracked to form a validation procedure. These specific cells were selected on two criteria:\n",
    "\n",
    "- The cells do not divide in the time-lapse of 30 images\n",
    "- The cells are clearly separable in the first image\n",
    "- We were sure that our manual tracking was correct (some cells interact in a way that makes it impossible to be entirely sure which ones match between frames)\n",
    "\n",
    "The procedure for this is straightforward; the cells are tracked one image at a time. The center coordinates of the five cell are estimated in the last image. The validation algorithm will then check the outcome of the automatic tracking of the five cells (labeled 2, 7, 9, 11, 15) by comparing the mean coordinate of the resulting cell with the manually estimated mean coordinate. Some small error will be allowed, since this could result from a slightly wrong estimation of the mean coordinate. However, if the error exceeds a dynamic threshold in either the x or y direction, the cell tracking was off. The dynamic threshold will be set to 0.25 * diameter of the cell in the given direction. In this way, large cells can have a larger manual error by estimating the mean. The hand-labeled results are stored in tuples of coordinates of the form [((x_cell2_image1, y_cell2_image1), (x_cell2_image30, y_cell1_image30)), ...]. The first image (number 1) is excluded, since it seems to contain overlapping cells, and that makes it hard to track manually. Image 5 was discarded, because it divides into two cells after image 20, so our tracking algorithm will track only one of the resulting cells. \n",
    "\n",
    "Our validation algorithm only shows the number of correctly tracked cells. We could show the Euclidean distance between the true cell and predicted cell in image 30, but in our opinion this is not descriptive, since one mistake between two images can lead to a large distance in the true cell and predicted cell, which does not make the entire algorithm more wrong, just the small mistake between the two images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_results(original_cells, pred_cells):\n",
    "    \"\"\"\n",
    "    Coordinates in image 30 (manually estimated)\n",
    "        2: (194, 220)\n",
    "        7: (217, 362)\n",
    "        9: (371, 135)\n",
    "        11: (277, 212)\n",
    "        15: (346, 108)\n",
    "    \"\"\"\n",
    "    manual_indices = [1, 6, 8, 10, 14]\n",
    "    \n",
    "    # Select the 5 manually tracked cells. They have indices [1, 2, 3, 5, 6], and labels [index + 1 for index in indices].    \n",
    "    predictions = [p for p in pred_cells if p.idx in manual_indices]\n",
    "    originals = [p for p in original_cells if p.idx in manual_indices]\n",
    "    \n",
    "    true_coordinates = [(194, 220),\n",
    "                        (217, 362),\n",
    "                        (371, 135),\n",
    "                        (277, 212),\n",
    "                        (346, 108)]\n",
    "\n",
    "    correct = []\n",
    "    for orig, true, pred in zip(originals, true_coordinates, predictions):\n",
    "        err_x = abs(true[0] - pred.mean_x)\n",
    "        err_y = abs(true[1] - pred.mean_y)\n",
    "        print(f\"Error X and Y: ({round(err_x, 2)}, {round(err_y, 2)}). Correct: {bool((err_x < 0.25 * orig.width and err_y < 0.25 * orig.height))}\")\n",
    "        correct.append(bool(err_x < 0.25 * orig.width and err_y < 0.25 * orig.height))\n",
    "        \n",
    "    return sum(correct) / len(correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Develop an algorithm that can trace the cells over the time-lapse sequence. It is\n",
    "important is to make a choice for criteria by which you can determine that a cell in a\n",
    "next frame is the same cell in the current frame. Your algorithm is based on that\n",
    "criterion, or those criteria. This should be clearly stated in the explanation of the\n",
    "algorithm\n",
    "\n",
    "The algorithm developed iteratively finds the best match for the 15 selected cells in the previous frame. This means that 15 cells are selected once (in image 1). After this, the 15 best matches with image 1 will be found in image 2, after which the 15 best matches with image 3 are found in image 2. This is relatively straightforward.\n",
    "\n",
    "However, determining the closest match out of 50-150 segmentation results is not. This operation is fully dependent on the distance function that determines the relative distance between two cells, in Cell.distance. Note that this function is relative, so it does not have to express a spatial distance. \n",
    "\n",
    "For the first version of the algorithm, the distance function was simply the euclidean distance between the mean positions of the two cells. The results can be seen below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cell:\n",
    "    def __init__(self, contour, idx):\n",
    "        self.idx = idx\n",
    "        self.contour = contour\n",
    "        self.mean_x = np.mean(contour[:, :, 0])\n",
    "        self.mean_y = np.mean(contour[:, :, 1])\n",
    "        self.area = cv2.contourArea(contour)\n",
    "        self.width = max(contour[:, :, 0]) - min(contour[:, :, 0])\n",
    "        self.height = max(contour[:, :, 1]) - min(contour[:, :, 1])\n",
    "        \n",
    "    def features(self):\n",
    "        return  np.array([self.contour,\n",
    "                          self.mean_x,\n",
    "                          self.mean_y,\n",
    "                          self.area,\n",
    "                          self.width,\n",
    "                          self.height]).astype(np.uint16)\n",
    "    \n",
    "    def to_contour(self):\n",
    "        return self.contour\n",
    "    \n",
    "    def distance(self, cell):\n",
    "        return self.euclidean_distance(cell)\n",
    "    \n",
    "    def euclidean_distance(self, cell):\n",
    "        \"\"\"\n",
    "        Return the Squared Euclidean Distance between this and another cell\n",
    "        Since it is a relative distance, no square root is applied\n",
    "        \"\"\"\n",
    "        return abs(self.mean_x - cell.mean_x) ** 2 + abs(self.mean_y - cell.mean_y) ** 2\n",
    "    \n",
    "    def copy(self, idx):\n",
    "        new = copy.deepcopy(self)\n",
    "        new.idx = idx\n",
    "        return new\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Cell {self.idx}, mean {self.mean_x, self.mean_y}\\n\"\n",
    "         \n",
    "    \n",
    "def trace_cells_over_image(images, plot=False):\n",
    "    # Segment the first image and create Cell objects\n",
    "    segmentations_first_image = segment_cells(images[0], plot=False)\n",
    "    sorted_segmentations = sorted(segmentations_first_image, key=cv2.contourArea, reverse=True)\n",
    "    \n",
    "    segmented_cells = [Cell(cell, i) for i, cell in enumerate(sorted_segmentations)]\n",
    "        \n",
    "    # Select the 15 largest cells from the first image\n",
    "    tracking_cells = segmented_cells[:15]\n",
    "    \n",
    "    # Apply the tracking algorithm. \n",
    "    # Segment all images\n",
    "    segmentation_all_images = [tracking_cells] + [[Cell(cell, i) for i, cell in enumerate(segment_cells(images[j], plot=False))] for j in range(1, len(images))]\n",
    "    \n",
    "    # Select the 15 cells iteratively by computing the distance for each cell in the next image\n",
    "    for i in range(len(segmentation_all_images) - 1):\n",
    "        current_seg = segmentation_all_images[i]\n",
    "        next_seg = segmentation_all_images[i + 1]\n",
    "        new_next_seg = []\n",
    "        \n",
    "        # Find closest match for each cell\n",
    "        for cell in current_seg:\n",
    "            match = min(next_seg, key=lambda l: cell.distance(l))\n",
    "            new_next_seg.append(match.copy(cell.idx))\n",
    "            next_seg.remove(match)\n",
    "        \n",
    "        segmentation_all_images[i + 1] = new_next_seg\n",
    "        \n",
    "    colored_results = []\n",
    "    gray_results = []\n",
    "    \n",
    "    for i in range(len(segmentation_all_images)):\n",
    "        colored, gray = plot_segmentation(images[i], [c.to_contour() for c in segmentation_all_images[i]], number=i+1, show=plot)\n",
    "        colored_results.append(colored)\n",
    "        gray_results.append(gray)\n",
    "    \n",
    "    return colored_results, gray_results, tracking_cells, [c for c in segmentation_all_images[-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3:\n",
    "Execute first version of algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring it all together\n",
    "colored_results, gray_results, original_cells, predicted_cells = trace_cells_over_image(images_ctrl, plot=False)\n",
    "validate_results(original_cells, predicted_cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.3\n",
    "\n",
    "As we can see, all five cells are tracked correctly. We can try different distance functions to tune our algorithm, which in this case might not be wise since our validation returns 1.0, but in reality will often be better. In fact, many other cells are wrongly tracked it seems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_distance(self, cell):\n",
    "    \"\"\"\n",
    "    Create a new distance function that takes size into account.\n",
    "    We will eliminate any cells that differ too much in size by adding a large amount to the distance\n",
    "    \"\"\"\n",
    "    if not (0.75 < self.area / cell.area < 1.25):\n",
    "        return self.euclidean_distance(cell) + 1000000\n",
    "\n",
    "    return self.euclidean_distance(cell)\n",
    "\n",
    "# Change (monkeypatch) the new distance function in the Cell object\n",
    "# Cell.distance = new_distance\n",
    "\n",
    "# Execute algorithm again\n",
    "colored_results, gray_results, original_cells, predicted_cells = trace_cells_over_image(images_ctrl, plot=False)\n",
    "validate_results(original_cells, predicted_cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the report, we will therefore use the first (euclidean) distance function, since it seems to work best on our validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4\n",
    "The algorithm is applied to both set A and B. All intermediary results are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the images\n",
    "images_A = [np.mean(plt.imread(f\"images/MTLn3-Ctrl00{'0' + str(i) if i < 10 else i}.tif\"), -1).astype(np.uint16) for i in range(30)]\n",
    "images_B = [np.mean(plt.imread(f\"images/MTLn3+EGF00{'0' + str(i) if i < 10 else i}.tif\"), -1).astype(np.uint16) for i in range(30)]\n",
    "\n",
    "# Define results folders\n",
    "results_a = \"results/results-A/\"\n",
    "results_b = \"results/results-B/\"\n",
    "\n",
    "# Apply the algorithm\n",
    "colored_results_a, gray_results_a, _, _ = trace_cells_over_image(images_A, plot=False)\n",
    "colored_results_b, gray_results_b, _, _ = trace_cells_over_image(images_B, plot=False)\n",
    "\n",
    "# Save all results in correct folders\n",
    "for i in range(len(images_A)):\n",
    "    plt.imsave(f\"{results_a}colored/{i}.png\", colored_results_a[i].astype(np.uint8))\n",
    "    plt.imsave(f\"{results_a}gray/{i}.png\", gray_results_a[i].astype(np.uint8))\n",
    "    plt.imsave(f\"{results_b}colored/{i}.png\", colored_results_b[i].astype(np.uint8))\n",
    "    plt.imsave(f\"{results_b}gray/{i}.png\", gray_results_b[i].astype(np.uint8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "af91d67035eb2c99d854f402f9042ea7f1bd43754bb4a883e21408de431a319c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
